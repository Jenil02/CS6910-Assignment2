{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jenil02/CS6910-Assignment2/blob/main/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F302C4XXD-zh",
        "outputId": "20c5c287-0b71-4f13-f5b9-10676b132943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.6.1)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=9d80dddbbe73d763a81a274af04654d2c03859eafef05e81f4e50f37436ef890\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.19.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.2\n"
          ]
        }
      ],
      "source": [
        "pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "8mPFc08clOHs",
        "outputId": "916f7c1a-70db-4daa-83b6-6dd797bf89b9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import pathlib\n",
        "from torch.autograd import Variable\n",
        "import glob\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import random_split\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XxNNn8t0E2-4"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA1w98YBFsCj",
        "outputId": "c4850d28-8ded-4674-e1c9-84300cadfb36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UOEc7XSDTqhB"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/inaturalist_12K/train'\n",
        "test_path = '/content/drive/MyDrive/inaturalist_12K/val'\n",
        "\n",
        "def load_data(data_augment):\n",
        "  if data_augment == True:\n",
        "    transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Resize((256,256)),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.RandomVerticalFlip(),\n",
        "      transforms.RandomRotation(30)\n",
        "    ])\n",
        "  else:\n",
        "    transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Resize((256,256)),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "  trainset = torchvision.datasets.ImageFolder(train_path, transform=transform)\n",
        "\n",
        "  testset = torchvision.datasets.ImageFolder(test_path, transform=transform)\n",
        "\n",
        "  return trainset, testset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3kfwK9SKTWP",
        "outputId": "2c5f7198-bac0-41c5-b39e-62192006d934"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(load_data(True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XyDfTmD7F3Ax"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "\n",
        "  def __init__(self, num_classes, num_filters, kernel_size, droprate, batch_norm, filter_org, activation_fn, data_augment):\n",
        "    super(ConvNet,self).__init__()\n",
        "      \n",
        "    #Output size after convolution filter\n",
        "    #((w-f+2P)/s) + 1\n",
        "\n",
        "    # self.conv1 = nn.Conv3d(3, 12, kernel_size=3, stride=1, padding=1)\n",
        "    #shape = (32, 12, 256, 256)\n",
        "    self.num_filters = num_filters\n",
        "    self.kernel_size = kernel_size\n",
        "    self.num_classes = num_classes\n",
        "    self.droprate = droprate\n",
        "    self.batch_norm = batch_norm\n",
        "    self.activation = activation_fn\n",
        "    self.filter_org = filter_org\n",
        "    # self.train_dir = train_dir\n",
        "    # self.test_dir = test_dir\n",
        "    self.data_augment = data_augment\n",
        "\n",
        "    size = (32, 3, 256, 256)\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 3*num_filters, kernel_size, stride=1, padding=1)\n",
        "    self.batch_norm1 = nn.BatchNorm2d(3*num_filters)\n",
        "    self.pool = nn.MaxPool2d(3, stride=2)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(3*num_filters, 3*num_filters*(filter_org), kernel_size, stride=1, padding=1)\n",
        "    self.batch_norm2 = nn.BatchNorm2d(3*num_filters*(filter_org))\n",
        "\n",
        "    self.conv3 = nn.Conv2d(3*num_filters*(filter_org), 3*num_filters*(filter_org**2), kernel_size, stride=1, padding=1)\n",
        "    self.batch_norm3 = nn.BatchNorm2d(3*num_filters*(filter_org**2))\n",
        "\n",
        "    self.conv4 = nn.Conv2d(3*num_filters*(filter_org**2), 3*num_filters*(filter_org**3), kernel_size, stride=1, padding=1)\n",
        "    self.batch_norm4 = nn.BatchNorm2d(3*num_filters*(filter_org**3))\n",
        "\n",
        "    self.conv5 = nn.Conv2d(3*num_filters*(filter_org**3), 3*num_filters*(filter_org**4), kernel_size, stride=1, padding=1)\n",
        "    self.batch_norm5 = nn.BatchNorm2d(3*num_filters*(filter_org**4))\n",
        "    \n",
        "    self.flatten = nn.Flatten(start_dim=1, end_dim = -1)\n",
        "\n",
        "    for i in range(5):\n",
        "      m = nn.Conv2d(3, 30, kernel_size, stride=1, padding=1)\n",
        "      input = torch.randn(32, 3, size[2], size[3])\n",
        "      output = m(input)\n",
        "      n = nn.MaxPool2d(3, stride=2)\n",
        "      output = n(output)\n",
        "      size = output.shape\n",
        "      input = output\n",
        "\n",
        "    self.fc1 = nn.Linear(3*num_filters*(filter_org**4)*size[2]*size[3], num_classes)\n",
        "    self.dropout = nn.Dropout(droprate)\n",
        "    self.fc2 = nn.Linear(num_classes, num_classes)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def activation_fn(self,x):\n",
        "    if self.activation == 'ReLU':\n",
        "      return nn.functional.relu(x)\n",
        "    if self.activation == 'GELU':\n",
        "      return nn.functional.gelu(x)\n",
        "    if self.activation == 'SiLU':\n",
        "      return nn.functional.silu(x)\n",
        "    if self.activation == 'Mish':\n",
        "      return nn.functional.mish(x)\n",
        "    if self.activation == 'LeakyReLU':\n",
        "      return nn.functional.leaky_relu(x)\n",
        "\n",
        "  def data_load(self):\n",
        "    trainset, testset = load_data(self.data_augment)\n",
        "\n",
        "    test_abs = int(len(trainset)*0.8)\n",
        "    train_subset, val_subset = random_split(trainset, [test_abs, len(trainset)-test_abs])\n",
        "\n",
        "    trainloader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "    valloader = DataLoader(val_subset, batch_size=32, shuffle=True)\n",
        "    return train_subset, val_subset, trainloader, valloader, testset\n",
        "    \n",
        "\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    cur_in = inputs\n",
        "    \n",
        "    cur_out = self.conv1(cur_in)\n",
        "    if self.batch_norm == True:\n",
        "      cur_out = self.batch_norm1(cur_out)\n",
        "    cur_out = self.activation_fn(cur_out)\n",
        "    cur_out = self.pool(cur_out)\n",
        "\n",
        "    cur_out = self.conv2(cur_out)\n",
        "    if self.batch_norm == True:\n",
        "      cur_out = self.batch_norm2(cur_out)\n",
        "    cur_out = self.activation_fn(cur_out)\n",
        "    cur_out = self.pool(cur_out)\n",
        "\n",
        "\n",
        "    cur_out = self.conv3(cur_out)\n",
        "    if self.batch_norm == True:\n",
        "      cur_out = self.batch_norm3(cur_out)\n",
        "    cur_out = self.activation_fn(cur_out)\n",
        "    cur_out = self.pool(cur_out)\n",
        "    \n",
        "    cur_out = self.conv4(cur_out)\n",
        "    if self.batch_norm == True:\n",
        "      cur_out = self.batch_norm4(cur_out)\n",
        "    cur_out = self.activation_fn(cur_out)\n",
        "    cur_out = self.pool(cur_out)\n",
        "\n",
        "    cur_out = self.conv5(cur_out)\n",
        "    if self.batch_norm == True:\n",
        "      cur_out = self.batch_norm5(cur_out)\n",
        "    cur_out = self.activation_fn(cur_out)\n",
        "    cur_out = self.pool(cur_out)\n",
        "\n",
        "    cur_out = self.flatten(cur_out)\n",
        "\n",
        "    cur_out = self.dropout(cur_out)\n",
        "\n",
        "    cur_out = self.fc1(cur_out)\n",
        "\n",
        "    cur_out = self.fc2(cur_out)\n",
        "\n",
        "    cur_out = self.softmax(cur_out)\n",
        "    return cur_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "M7N6g9CkCnMI"
      },
      "outputs": [],
      "source": [
        "def train_val(model, optimizer, num_epochs):\n",
        "\n",
        "  train_subset, val_subset, trainloader, valloader, testset = model.data_load()\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  train_count = len(train_subset)\n",
        "  val_count = len(val_subset)\n",
        "  optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_accuracy = 0.0\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for i, (images,labels) in enumerate(trainloader):\n",
        "      if torch.cuda.is_available():\n",
        "        images = Variable(images.cuda())\n",
        "        labels = Variable(labels.cuda())\n",
        "      else:\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(images)\n",
        "      loss = loss_function(outputs,labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.cpu().data*images.size(0)\n",
        "      _,prediction = torch.max(outputs.data, 1)\n",
        "\n",
        "      train_accuracy += int(torch.sum(prediction == labels.data))\n",
        "\n",
        "    train_accuracy = train_accuracy/train_count\n",
        "    train_loss = train_loss/train_count\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    val_loss = 0.0\n",
        "    val_accuracy = 0.0\n",
        "\n",
        "    for i, (images,labels) in enumerate(valloader):\n",
        "      with torch.no_grad():\n",
        "        if torch.cuda.is_available():\n",
        "          images = Variable(images.cuda())\n",
        "          labels = Variable(labels.cuda())\n",
        "        else:\n",
        "          images = Variable(images)\n",
        "          labels = Variable(labels)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = loss_function(outputs,labels)\n",
        "\n",
        "        val_loss += loss.cpu().data*images.size(0)\n",
        "        _,prediction = torch.max(outputs.data, 1)\n",
        "\n",
        "        val_accuracy += int(torch.sum(prediction == labels.data))\n",
        "\n",
        "    val_accuracy = val_accuracy/val_count\n",
        "    val_loss = val_loss/val_count\n",
        "\n",
        "    print({\"train_loss\": train_loss, \"val_loss\": val_loss, \"train_accuracy\": train_accuracy, \"val_accuracy\": val_accuracy})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEnCzAGUCZ2a"
      },
      "outputs": [],
      "source": [
        "model = ConvNet(10, 3, 5, 0.1, True, 1, 'ReLU', True).to(device)\n",
        "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "train_val(model, optimizer, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "9Zkm7VRwGgMG",
        "outputId": "f527613e-81f3-4ebe-f728-65e2b2c2726b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: vltg14uy\n",
            "Sweep URL: https://wandb.ai/jenilsheth/iNaturalist/sweeps/vltg14uy\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w04piyde with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: Mish\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdroprate: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 32\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230414_163914-w04piyde</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jenilsheth/iNaturalist/runs/w04piyde' target=\"_blank\">swift-sweep-1</a></strong> to <a href='https://wandb.ai/jenilsheth/iNaturalist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jenilsheth/iNaturalist/sweeps/vltg14uy' target=\"_blank\">https://wandb.ai/jenilsheth/iNaturalist/sweeps/vltg14uy</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jenilsheth/iNaturalist' target=\"_blank\">https://wandb.ai/jenilsheth/iNaturalist</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/jenilsheth/iNaturalist/sweeps/vltg14uy' target=\"_blank\">https://wandb.ai/jenilsheth/iNaturalist/sweeps/vltg14uy</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jenilsheth/iNaturalist/runs/w04piyde' target=\"_blank\">https://wandb.ai/jenilsheth/iNaturalist/runs/w04piyde</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def run():\n",
        "  wandb.init()\n",
        "  config = wandb.config\n",
        "  model = ConvNet(10, config.num_filters,  config.kernel_size, config.droprate, config.batch_norm, config.filter_org, config.activation, config.data_augment).to(device)\n",
        "  optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "  print('hi')\n",
        "  train_val(model, optimizer, 5)\n",
        "  print('bye')\n",
        "  wandb.finish()\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'random', #grid, random\n",
        "    'metric': {\n",
        "        'goal' : 'maximize',\n",
        "        'name': 'val_accuracy'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'num_filters': {\n",
        "            'values': [16, 32, 64, 128]\n",
        "        },\n",
        "        'filter_org': {\n",
        "            'values': [0.5, 1, 2, 3]\n",
        "        },\n",
        "        'kernel_size': {\n",
        "            'values': [3, 5, 7]\n",
        "        },\n",
        "        'droprate': {\n",
        "            'values': [0, 0.1, 0.2, 0.3]\n",
        "        },\n",
        "        'batch_norm': {\n",
        "            'values': [True, False]\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['ReLU', 'GELU', 'SiLU', 'Mish', 'LeakyReLU']\n",
        "        },\n",
        "        'data_augment': {\n",
        "            'values': [True, False]\n",
        "        },\n",
        "        \n",
        "        }\n",
        "    }\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"iNaturalist\")\n",
        "wandb.agent(sweep_id, run, count=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "12EqeeNeZ5CUCGRJJ-2VRY4a9nq9eJlXd",
      "authorship_tag": "ABX9TyPrVPTOZzbT8YJbz7kGSydK",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}